{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "import bs4 as bs\n",
    "import json\n",
    "import traceback\n",
    "import re\n",
    "import asyncio\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import glob, os, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_visit_rfi = {}\n",
    "load_more = \"btn-load-more alt more-loader\"\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=\"/home/aims/Downloads/chromedriver\")\n",
    "driver.get(\"http://sw.rfi.fr/afrika/all/\")\n",
    "loadmore = driver.find_element_by_class_name(\"cookie-bar_hide\")\n",
    "loadmore.click()\n",
    "while True:\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        loadmore = WebDriverWait(driver, 120).until(EC.visibility_of_element_located((By.CLASS_NAME, \"btn-load-more\")))\n",
    "        loadmore.click()\n",
    "        time.sleep(1)\n",
    "    except Exception:\n",
    "        print(\"Reached bottom of page\")\n",
    "        traceback.print_exc()\n",
    "        break\n",
    "\n",
    "soup_rfi = bs.BeautifulSoup(driver.page_source,'html.parser')\n",
    "news_links = soup_rfi.find_all('a', href=True)\n",
    "len(news_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Save the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, filename=\"final.txt\"):\n",
    "    with open(filename, 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(\"%s\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_final = []\n",
    "\n",
    "for a in news_links:\n",
    "    if a.text:\n",
    "        link = a[\"href\"]\n",
    "        matching = re.match(r'^\\/michezo\\/\\d+\\w+', link)\n",
    "        if matching:\n",
    "            new_link = 'http://sw.rfi.fr'+link\n",
    "            news_final.append(new_link)\n",
    "\n",
    "save_data(news_final, 'news_final.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "def get_urls():\n",
    "    with open(\"news_final.txt\") as f:\n",
    "        for line in f:\n",
    "            urls.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_urls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get links content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = []\n",
    "\n",
    "def do_req(url):\n",
    "    return requests.get(url, headers=headers)\n",
    "\n",
    "async def main():\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        futures = [\n",
    "            loop.run_in_executor(\n",
    "                executor, \n",
    "                do_req,\n",
    "                url\n",
    "            )\n",
    "            for url in urls\n",
    "        ]\n",
    "        for response in await asyncio.gather(*futures):\n",
    "            page_soup = bs.BeautifulSoup(response.text, 'html.parser')\n",
    "            news_title = page_soup.select_one('article h1')\n",
    "            news_title = news_title.text if news_title else ''\n",
    "            news_body = page_soup.select_one('article div[itemprop=\"articleBody\"]')\n",
    "            news_body = news_body.text if news_body else ''\n",
    "            final_data.append({'title' : news_title, 'content' : news_body})\n",
    "\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = []\n",
    "for item in final_data:\n",
    "    if item['content'] != '':\n",
    "        full.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_full.json', 'w') as fout:\n",
    "    json.dump(full, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.json\", \"r\") as read_file:\n",
    "    data = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sw_final.txt', 'w') as file:\n",
    "    for item in data:\n",
    "        join_title_description = item['title'] + '\\n\\n' + item['content'] + '\\n\\n'\n",
    "        file.write(join_title_description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
